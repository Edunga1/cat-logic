{"componentChunkName":"component---src-components-gatsby-templates-wiki-tsx","path":"/wiki/인공지능/용어/추론 애플리케이션/","result":{"data":{"markdownRemark":{"headings":[{"value":"추론 애플리케이션"}],"fields":{"relatedDocs":[]},"tableOfContents":"<ul>\n<li><a href=\"#%EC%B6%94%EB%A1%A0-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98\">추론 애플리케이션</a></li>\n</ul>","html":"<h1 id=\"추론-애플리케이션\" style=\"position:relative;\"><a href=\"#%EC%B6%94%EB%A1%A0-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98\" aria-label=\"추론 애플리케이션 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>추론 애플리케이션</h1>\n<p>\"LLM 추론\"과 같이 모델에 대한 추론을 하였다는 표현을 종종 본다.</p>\n<p>아직 모델 추론이 무엇을 의미하는 지는 잘 모르겠지만 몇몇 아티클을 읽어보고 감을 잡은 것을 정리해본다.</p>\n<p>참고한 아티클은 다음과 같다.</p>\n<ol>\n<li><a href=\"https://news.hada.io/topic?id=11847\">Rust+WASM으로 이기종 Edge에서 빠르고 포터블한 Llama2 추론 실행하기</a></li>\n<li><a href=\"https://news.hada.io/topic?id=8682\">llama.cpp - 페이스북의 LLaMA 모델을 순수 C/C++로 추론하기</a></li>\n<li><a href=\"https://news.hada.io/topic?id=10379\">LLaMa.cpp가 어떻게 가능할까?</a></li>\n</ol>\n<blockquote>\n<p>Python 종속성은 엄청남. Python 또는 PyTorch용 Docker 이미지는 일반적으로 몇 GB 또는 수십 GB에 달하며, 이는 엣지 서버나 디바이스에서 AI 추론을 수행할 때 특히 문제가 됨</p>\n</blockquote>\n<blockquote>\n<p>초경량: 추론 애플리케이션은 모든 종속성을 포함하여 2MB에 불과. 이는 일반적인 PyTorch 컨테이너 크기의 1%도 되지 않음</p>\n</blockquote>\n<blockquote>\n<p>LLaMa 추론 코드를 순수 C++로 재작성한 LLaMA.cpp 덕분에 Pixel5, M2 맥북프로, Raspberry Pi 등 다양한 하드웨어에서 실행 가능</p>\n</blockquote>\n<p>아마도 모델을 사용하는 것을 추론이라고 하는 것 같다.\nPyTorch나 Tensorflow 같은 프레임워크를 사용하여 모델을 불러와서 사용할 수 있다.\n하지만 이러한 프레임워크는 학습을 위한 라이브러리를 제공하거나, 다양한 모델에 대한 기능을 제공하기 때문에 너무 무겁다.\n그래서 전용 \"추론 애플리케이션\"은 가벼운 환경에서 동작케 하는 것이 목적인 것으로 보인다.</p>\n<blockquote>\n<p>맥북에서 LLaMA 모델을 4-bit 양자화하여 실행하는 것을 목표</p>\n</blockquote>\n<p>llama.cpp는 고성능 서버 컴퓨팅이 아닌 맥북과 같은 워크스테이션에서도 실행하는 것이 목적이다.\n첫 번째 아티클은 WASM으로 브라우저에서도 모델 추론을 수행할 수 있도록 하는 것이 목적이다.\n모두 가벼운 컴퓨팅 환경에서 실행 가능케 한다.</p>"}},"pageContext":{"id":"345bc108-0c15-5d49-9c77-7f4cc28c05fd"}},"staticQueryHashes":[],"slicesMap":{}}