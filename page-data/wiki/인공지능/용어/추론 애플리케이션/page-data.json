{"componentChunkName":"component---src-components-gatsby-templates-wiki-tsx","path":"/wiki/인공지능/용어/추론 애플리케이션/","result":{"data":{"markdownRemark":{"headings":[{"value":"추론 애플리케이션"}],"fields":{"slug":"/인공지능/용어/추론 애플리케이션/","relatedDocs":[{"slug":"reverse-engineering","similarity":0.8019663678},{"slug":"design-pattern","similarity":0.7699598188},{"slug":"kubernetes","similarity":0.7733900954},{"slug":"markdown","similarity":0.8150379901},{"slug":"kotlin","similarity":0.7817842119},{"slug":"jira","similarity":0.7563653719},{"slug":"book","similarity":0.799406193},{"slug":"vim","similarity":0.7909734274},{"slug":"hardware","similarity":0.8121119869},{"slug":"network","similarity":0.7694930374},{"slug":"architecture","similarity":0.8299615029},{"slug":"python","similarity":0.8016410677},{"slug":"physics","similarity":0.7764465501},{"slug":"experience-review","similarity":0.8041746107},{"slug":"2016-08-27-gdg-webtech-workshop-nnn","similarity":0.8192449325},{"slug":"css","similarity":0.7598410585},{"slug":"redis","similarity":0.747439648},{"slug":"embeddings","similarity":0.852443725},{"slug":"clean-code","similarity":0.7892523687},{"slug":"scp","similarity":0.7114077033},{"slug":"vimwiki","similarity":0.8033763926},{"slug":"cat-logic","similarity":0.82017959},{"slug":"rust","similarity":0.7904100679},{"slug":"github","similarity":0.7792010645},{"slug":"2016-11-16-google-campus-two-things-you-must-keep-in-google-play","similarity":0.8081220029},{"slug":"jargon","similarity":0.7984494116},{"slug":"windows","similarity":0.7783696829},{"slug":"test-driven-development","similarity":0.7838526507},{"slug":"javascript","similarity":0.8001595908},{"slug":"c-sharp","similarity":0.7529716847},{"slug":"linux","similarity":0.8149089792},{"slug":"testing","similarity":0.7854689541},{"slug":"valve","similarity":0.7846266567},{"slug":"sentry","similarity":0.7749499158},{"slug":"continuous-integration-and-deployment","similarity":0.781775677},{"slug":"object-oriented-programming","similarity":0.7890045082},{"slug":"angularjs","similarity":0.7832209981},{"slug":"webgl","similarity":0.8086516208},{"slug":"google-analytics","similarity":0.755882983},{"slug":"quotation","similarity":0.7969346946},{"slug":"airflow","similarity":0.8002853469},{"slug":"reactjs","similarity":0.7993217662},{"slug":"unicode","similarity":0.7693666336},{"slug":"mac-os","similarity":0.7945112045},{"slug":"sfml","similarity":0.796706999},{"slug":"inspiration","similarity":0.8370550445},{"slug":"spring-framework","similarity":0.7897405901},{"slug":"data-structure","similarity":0.7678153643},{"slug":"java","similarity":0.7709982595},{"slug":"logging","similarity":0.7633527838},{"slug":"swagger","similarity":0.7591689909},{"slug":"vimenter-2023","similarity":0.8073046151},{"slug":"reactive-extensions","similarity":0.7393790442},{"slug":"windows-subsystem-for-linux","similarity":0.7980302333},{"slug":"mail","similarity":0.7670843653},{"slug":"unity3d","similarity":0.7776840799},{"slug":"web","similarity":0.8077346784},{"slug":"code-review","similarity":0.8042900398},{"slug":"git","similarity":0.806639064},{"slug":"language-server-protocol","similarity":0.8027756622},{"slug":"algorithm-practice","similarity":0.7360590514},{"slug":"programming-paradigm","similarity":0.8046775996},{"slug":"elasticsearch","similarity":0.815376838},{"slug":"idea-methodology","similarity":0.8165919771},{"slug":"software-development","similarity":0.8239955613},{"slug":"docker","similarity":0.8133784658},{"slug":"aws","similarity":0.7970193426},{"slug":"programming-convention","similarity":0.8081651363},{"slug":"devops","similarity":0.817987398},{"slug":"html-canvas","similarity":0.7413493783},{"slug":"programming-philosophy","similarity":0.8141142735},{"slug":"jetbrains","similarity":0.8226993932},{"slug":"ionic-framework","similarity":0.7623672634},{"slug":"data-analysis","similarity":0.7996987424},{"slug":"game","similarity":0.8009282076},{"slug":"machine-learning","similarity":0.8914529455},{"slug":"nodejs","similarity":0.8129987059},{"slug":"shell","similarity":0.791891648},{"slug":"gatsbyjs","similarity":0.8222254222},{"slug":"html","similarity":0.7587441623},{"slug":"computer-graphics","similarity":0.7756334467},{"slug":"vuejs","similarity":0.7539454714},{"slug":"tools","similarity":0.7961814021},{"slug":"crontab","similarity":0.7781811584},{"slug":"database","similarity":0.8094301579},{"slug":"추론 애플리케이션","similarity":1}]},"tableOfContents":"<ul>\n<li><a href=\"#%EC%B6%94%EB%A1%A0-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98\">추론 애플리케이션</a></li>\n</ul>","html":"<h1 id=\"추론-애플리케이션\" style=\"position:relative;\"><a href=\"#%EC%B6%94%EB%A1%A0-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98\" aria-label=\"추론 애플리케이션 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>추론 애플리케이션</h1>\n<p>\"추론하다(inference)\"의 의미는 뭘까?</p>\n<p>LLM 관련 글을 읽다보면 모델 추론이라는 표현을 종종 본다.<br>\n의미를 아직 잘 모르겠지만 몇몇 아티클을 읽어보고 추정하는 내용을 정리해본다.</p>\n<p>참고한 아티클은 다음과 같다.</p>\n<ol>\n<li><a href=\"https://news.hada.io/topic?id=11847\">Rust+WASM으로 이기종 Edge에서 빠르고 포터블한 Llama2 추론 실행하기</a></li>\n<li><a href=\"https://news.hada.io/topic?id=8682\">llama.cpp - 페이스북의 LLaMA 모델을 순수 C/C++로 추론하기</a></li>\n<li><a href=\"https://news.hada.io/topic?id=10379\">LLaMa.cpp가 어떻게 가능할까?</a></li>\n</ol>\n<blockquote>\n<p>Python 종속성은 엄청남. Python 또는 PyTorch용 Docker 이미지는 일반적으로 몇 GB 또는 수십 GB에 달하며, 이는 엣지 서버나 디바이스에서 AI 추론을 수행할 때 특히 문제가 됨</p>\n</blockquote>\n<blockquote>\n<p>초경량: 추론 애플리케이션은 모든 종속성을 포함하여 2MB에 불과. 이는 일반적인 PyTorch 컨테이너 크기의 1%도 되지 않음</p>\n</blockquote>\n<blockquote>\n<p>LLaMa 추론 코드를 순수 C++로 재작성한 LLaMA.cpp 덕분에 Pixel5, M2 맥북프로, Raspberry Pi 등 다양한 하드웨어에서 실행 가능</p>\n</blockquote>\n<p>아마도 모델을 사용하는 것을 추론이라고 하는 것 같다.\nPyTorch나 Tensorflow 같은 프레임워크를 사용하여 모델을 불러와서 사용할 수 있다.\n하지만 이러한 프레임워크는 학습을 위한 라이브러리를 제공하거나, 다양한 모델에 대한 기능을 제공하기 때문에 너무 무겁다.\n그래서 전용 \"추론 애플리케이션\"은 가벼운 환경에서 동작케 하는 것이 목적인 것으로 보인다.</p>\n<blockquote>\n<p>맥북에서 LLaMA 모델을 4-bit 양자화하여 실행하는 것을 목표</p>\n</blockquote>\n<p>llama.cpp는 고성능 서버 컴퓨팅이 아닌 맥북과 같은 워크스테이션에서도 실행하는 것이 목적이다.\n첫 번째 아티클은 WASM으로 브라우저에서도 모델 추론을 수행할 수 있도록 하는 것이 목적이다.\n모두 가벼운 컴퓨팅 환경에서 실행 가능케 한다.</p>\n<hr>\n<p><a href=\"https://news.hada.io/topic?id=11980\">https://news.hada.io/topic?id=11980</a></p>\n<blockquote>\n<p>AI가 더욱 주류가 되면서, 우리는 추론(inference) 작업에서 발생하는 부하가 훨씬 더 컴퓨팅 집약적이 될 것으로 기대합니다. 1억 명의 GPT-4 사용자를 서비스하는 데 드는 비용은 모델을 훈련하는 데 소요된 비용의 4배가 될 수 있습니다.</p>\n</blockquote>\n<p>추론을 통해 사용자에게 서비스한다. 그 비용이 학습에 소요되는 비용보다 커진다.</p>\n<hr>\n<p><a href=\"https://www.aitimes.com/news/articleView.html?idxno=155470\">LLM 추론 속도 300배까지 향상...'패스트 피드 포워드' 아키텍처 공개</a></p>\n<blockquote>\n<p>... (중략)\n연구진은 이 기술을 검증하기 위해 트랜스포머 기반의 구글 '버트(BERT)'의 피드 포워드 레이어를 FFF로 대체한 ‘패스트 보트(Fast BERT)’ 모델을 개발했다.</p>\n</blockquote>\n<p>언어 모델의 구조를 잘 모르겠지만, 기존 모델을 크게 변경하지 않고서도 적용이 가능한 모양이다.</p>\n<blockquote>\n<p>특히 연구진은 FFF 네트워크를 LLM에 통합하면 엄청난 가속 가능성이 있다고 주장했다. 예를 들어 'GPT-3'에서 각 트랜스포머의 피드 포워드 네트워크는 4만9152개의 뉴런으로 구성되지만, 15층 깊이의 FFF 네트워크로 대체할 경우 총 6만5536개의 뉴런을 포함하지만 실제 추론에는 GPT-3 뉴런의 약 0.03%에 해당하는 16개만 사용한다.</p>\n</blockquote>\n<p>추론에 사용하는 뉴런의 수를 줄여서 속도를 높였다고 한다.</p>\n<blockquote>\n<p>단일 'A6000' GPU에서 단 하루 동안 훈련한 패스트 버트 모델은 버트 모델 성능의 최소 96%를 유지했으며, 가장 뛰어난 실험 결과에서는 피드 포워드 레이어의 뉴런을 고작 0.3%만 사용하면서 기존 버트 모델과 동일한 성능을 보였다.</p>\n</blockquote>\n<p>단, 성능 저하가 있다고 한다. 그래도 속도 향상이 큰 것에 비해 성능 저하는 매우 적은 편이다.</p>"}},"pageContext":{"id":"345bc108-0c15-5d49-9c77-7f4cc28c05fd"}},"staticQueryHashes":[],"slicesMap":{}}