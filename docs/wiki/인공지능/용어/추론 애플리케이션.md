---
created: 2023-11-14
---
# 추론 애플리케이션

"LLM 추론"과 같이 모델에 대한 추론을 하였다는 표현을 종종 본다.

아직 모델 추론이 무엇을 의미하는 지는 잘 모르겠지만 몇몇 아티클을 읽어보고 감을 잡은 것을 정리해본다.

참고한 아티클은 다음과 같다.

1. [Rust+WASM으로 이기종 Edge에서 빠르고 포터블한 Llama2 추론 실행하기](https://news.hada.io/topic?id=11847)
2. [llama.cpp - 페이스북의 LLaMA 모델을 순수 C/C++로 추론하기](https://news.hada.io/topic?id=8682)
3. [LLaMa.cpp가 어떻게 가능할까?](https://news.hada.io/topic?id=10379)

> Python 종속성은 엄청남. Python 또는 PyTorch용 Docker 이미지는 일반적으로 몇 GB 또는 수십 GB에 달하며, 이는 엣지 서버나 디바이스에서 AI 추론을 수행할 때 특히 문제가 됨

> 초경량: 추론 애플리케이션은 모든 종속성을 포함하여 2MB에 불과. 이는 일반적인 PyTorch 컨테이너 크기의 1%도 되지 않음

> LLaMa 추론 코드를 순수 C++로 재작성한 LLaMA.cpp 덕분에 Pixel5, M2 맥북프로, Raspberry Pi 등 다양한 하드웨어에서 실행 가능

아마도 모델을 사용하는 것을 추론이라고 하는 것 같다.
PyTorch나 Tensorflow 같은 프레임워크를 사용하여 모델을 불러와서 사용할 수 있다.
하지만 이러한 프레임워크는 학습을 위한 라이브러리를 제공하거나, 다양한 모델에 대한 기능을 제공하기 때문에 너무 무겁다.
그래서 전용 "추론 애플리케이션"은 가벼운 환경에서 동작케 하는 것이 목적인 것으로 보인다.

> 맥북에서 LLaMA 모델을 4-bit 양자화하여 실행하는 것을 목표

llama.cpp는 고성능 서버 컴퓨팅이 아닌 맥북과 같은 워크스테이션에서도 실행하는 것이 목적이다.
첫 번째 아티클은 WASM으로 브라우저에서도 모델 추론을 수행할 수 있도록 하는 것이 목적이다.
모두 가벼운 컴퓨팅 환경에서 실행 가능케 한다.

---

https://news.hada.io/topic?id=11980

> AI가 더욱 주류가 되면서, 우리는 추론(inference) 작업에서 발생하는 부하가 훨씬 더 컴퓨팅 집약적이 될 것으로 기대합니다. 1억 명의 GPT-4 사용자를 서비스하는 데 드는 비용은 모델을 훈련하는 데 소요된 비용의 4배가 될 수 있습니다.

추론을 통해 사용자에게 서비스한다. 그 비용이 학습에 소요되는 비용보다 커진다.
